# ğŸš— Tesla Model 3 Hybrid RAG Assistant
**AI & Automation Internship Selection Challenge â€” Option 1**

---

## âœ… Task Chosen
I selected **Option 1: Hybrid Support Bot (RAG System)** from the challenge instructions.  
This task requires building a Retrieval-Augmented Generation (RAG) assistant that answers questions strictly based on a provided document â€” in this case, the **Tesla Model 3 Ownerâ€™s Manual** â€” without hallucinating or adding external information.

---

## ğŸ“˜ Project Overview
This RAG system:

- Parses the Tesla Model 3 Ownerâ€™s Manual (PDF) 
  https://www.tesla.com/ownersmanual/model3/en_us/Owners_Manual.pdf
- Extracts headings, chapters, and metadata
- Chunks and embeds content using **nomic-embed-text** (via Ollama)
- Stores vectors inside a **ChromaDB** persistent database
- Retrieves the most relevant sections for each query
- Builds a strict, grounded RAG prompt with hallucination prevention
- Generates answers using **llama3.1:8b-instruct-q4_K_M** (local)
- Provides both a **CLI interface** and a **Streamlit web UI**

##### This project fulfills all requirements for Option 1.

---

## ğŸ“ Project Structure

project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ec2-ug.pdf
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ parsing/
â”‚   â”‚   â”œâ”€â”€ pdf_reader.py
â”‚   â”‚   â”œâ”€â”€ heading_extractor.py
â”‚   â”‚   â””â”€â”€ text_cleaner.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â””â”€â”€ ingest.py
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ retriever.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ llama_client.py
â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ rag_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ cli_query.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ types.py
â”‚       â””â”€â”€ config.py
â”‚
â”œâ”€â”€ chroma_db/
â”‚
â”‚
â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ screenshots/
â”‚   â””â”€â”€ demo_video.mp4
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â””â”€â”€ .gitignore



## âš™ï¸ How to Set Up & Run the Project

#### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/YOUR_USERNAME/hybrid_rag_bot.git

#### 2ï¸âƒ£ Create Environment

conda create -n ragbot python=3.10 -y
conda activate ragbot

#### 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

#### 4ï¸âƒ£ Pull Required Models (Ollama)
ollama pull llama3.1
ollama pull nomic-embed-text

### ğŸ“˜ Step 1 â€” Ingest the Manual

Converts the PDF â†’ clean text â†’ chunks â†’ embeddings â†’ Chroma vector store.

python -m src.ingestion.ingest

### ğŸ” Step 2 â€” Test Retrieval
python -m src.retrieval.retriever

### ğŸ¤– Step 3 â€” Run RAG Pipeline (CLI)
python -m src.pipeline.rag_pipeline

### ğŸ–¥ï¸ Step 4 â€” Launch Streamlit Web Interface
streamlit run app/app.py


#### The UI includes:

- Chat interface

- Retrieved context preview

- Latency metrics

- Conversation memory

- Optional chapter filter

### ğŸ“š Why These Libraries & Models?

- LangChain 2025

- Used for modern, modular RAG orchestration (LCEL pipelines, prompt templates, retrieval flows).

- ChromaDB

- Lightweight, persistent vector store ideal for local/manual-based retrieval.

- PyMuPDF

- Reliable PDF extraction needed for structured parsing of the Tesla manual.

- Ollama + Llama 3.1

- Local LLM with no API cost, optimized for grounded question answering.

- nomic-embed-text

- Fast, high-quality embeddings suited for large manuals and technical documents.

- Streamlit

- Makes the assistant easy to demo and interact with through a web UI.

## ğŸ“¸ Screenshots

### Ingestion Completed
![ingestion](screenshots/ingestion1.png)
![ingestion](screenshots/ingestion2.png)
![ingestion](screenshots/ingestion3.png)

### Retriever Output
![retriever](screenshots/retriever.png)

### RAG Pipeline Answer
![rag](screenshots/rag_pipeline.png)

### CLI UI
![cli](screenshots/cli.png)

### Streamlit UI
![ui](screenshots/streamlit_ui.png)
