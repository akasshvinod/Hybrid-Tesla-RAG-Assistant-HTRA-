# ğŸš— **Tesla Model 3 Hybrid RAG Assistant**  
**AI & Automation Internship Selection Challenge â€” Option 1**

---

## âœ… **Task Chosen**
I selected **Option 1: Hybrid Support Bot (RAG System)** from the challenge instructions.  
This task requires building a Retrieval-Augmented Generation (RAG) assistant that answers questions strictly from a document â€” in this case, the **Tesla Model 3 Ownerâ€™s Manual** â€” with no hallucinations or external knowledge.

---

## ğŸ“˜ **Project Overview**
This project implements a fully offline, metadata-aware RAG system that:

- Parses the **Tesla Model 3 Ownerâ€™s Manual (PDF)**  
  https://www.tesla.com/ownersmanual/model3/en_us/Owners_Manual.pdf
- Extracts **chapters, headings, page numbers, and metadata**
- Cleans and chunks the manual for efficient retrieval  
- Embeds content using **nomic-embed-text** (via Ollama)
- Stores vectors inside a **persistent ChromaDB** database
- Retrieves relevant sections using **hybrid (vector + metadata) search**
- Builds a strict **grounded RAG prompt** with hallucination prevention
- Generates responses using **llama3.1:8b-instruct-q4_K_M** locally
- Provides a **CLI assistant** and a **Streamlit UI**

### â­ This project fulfills all requirements for **Option 1**.

---

### âš™ï¸ How to Set Up & Run the Project
#### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/akasshvinod/Hybrid-Tesla-RAG-Assistant-HTRA-.git

#### 2ï¸âƒ£ Create Environment
conda create -n ragbot python=3.10 -y
conda activate ragbot

#### 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

#### 4ï¸âƒ£ Pull Required Models (Ollama)
ollama pull llama3.1
ollama pull nomic-embed-text

#### ğŸ“˜ Step 1 â€” Ingest the Manual

Converts the PDF â†’ cleaned text â†’ chunks â†’ embeddings â†’ Chroma vector store.

python -m src.ingestion.ingest

#### ğŸ” Step 2 â€” Test Retrieval
python -m src.retrieval.retriever

#### ğŸ¤– Step 3 â€” Run RAG Pipeline (CLI)
python -m src.pipeline.rag_pipeline

#### ğŸ–¥ï¸ Step 4 â€” Launch Streamlit Web Interface
streamlit run app/app.py

The UI includes:

Chat interface

Retrieved context preview

Latency metrics

Conversation memory

Optional chapter filter

### ğŸ“š Why These Libraries & Models?
- LangChain 2025

- Modern LCEL pipelines

- Clean modular RAG orchestration

- ChromaDB

- Fast, persistent local vector store

- PyMuPDF

- Accurate PDF parsing for structured manuals

- Ollama + Llama 3.1

- Fully offline inference

- No API cost

- High grounding accuracy

- nomic-embed-text

- High-quality embeddings designed for documents

- Streamlit

- Quick and interactive UI

## ğŸ¥ Demo Video
Here is the demonstration video for the Tesla Model 3 Hybrid RAG Assistant:

ğŸ”— https://www.loom.com/share/48e669a811b3458abf6c472efdb9500f

